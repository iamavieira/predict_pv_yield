{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import rasterio.warp as rasteriowarp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SATELLITE_DATA_PATH = os.path.expanduser('~/data/EUMETSAT/reprojected_subsetted/')\n",
    "PV_DATA_FILENAME = os.path.expanduser('~/data/pvoutput.org/UK_PV_timeseries_batch.nc')\n",
    "PV_METADATA_FILENAME = os.path.expanduser('~/data/pvoutput.org/UK_PV_metadata.csv')\n",
    "\n",
    "DST_CRS = {\n",
    "    'ellps': 'WGS84',\n",
    "    'proj': 'tmerc',  # Transverse Mercator\n",
    "    'units': 'm'  # meters\n",
    "}\n",
    "\n",
    "# Geospatial boundary in Transverse Mercator projection (meters)\n",
    "SOUTH = 5513500\n",
    "NORTH = 6613500\n",
    "WEST =  -889500\n",
    "EAST =   410500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and convert PV metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metadata = pd.read_csv(PV_METADATA_FILENAME, index_col='system_id')\n",
    "pv_metadata.dropna(subset=['longitude', 'latitude'], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2548"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lat lons to Transverse Mercator\n",
    "pv_metadata['x'], pv_metadata['y'] = rasteriowarp.transform(\n",
    "    src_crs={'init': 'EPSG:4326'},\n",
    "    dst_crs=DST_CRS,\n",
    "    xs=pv_metadata['longitude'].values,\n",
    "    ys=pv_metadata['latitude'].values)\n",
    "\n",
    "# Filter 3 PV systems which apparently aren't in the UK!\n",
    "pv_metadata = pv_metadata[\n",
    "    (pv_metadata.x >= WEST) &\n",
    "    (pv_metadata.x <= EAST) &\n",
    "    (pv_metadata.y <= NORTH) &\n",
    "    (pv_metadata.y >= SOUTH)]\n",
    "\n",
    "len(pv_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalise PV power data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pv_power = xr.load_dataset(PV_DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_selected = pv_power.loc[dict(datetime=slice('2018-06-01', '2019-07-01'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df = pv_power_selected.to_dataframe().dropna(axis='columns', how='all')\n",
    "pv_power_df = pv_power_df.clip(lower=0, upper=5E7)\n",
    "pv_power_df.columns = [np.int64(col) for col in pv_power_df.columns]\n",
    "pv_power_df = pv_power_df.tz_localize('Europe/London').tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pv_power\n",
    "del pv_power_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of hand-crafted cleaning\n",
    "pv_power_df[30248]['2018-10-29':'2019-01-03'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only pick PV systems for which we have good metadata\n",
    "def align_pv_system_ids(pv_metadata, pv_power_df):\n",
    "    pv_system_ids = pv_metadata.index.intersection(pv_power_df.columns)\n",
    "    pv_system_ids = np.sort(pv_system_ids)\n",
    "\n",
    "    pv_power_df = pv_power_df[pv_system_ids]\n",
    "    pv_metadata = pv_metadata.loc[pv_system_ids]\n",
    "    return pv_metadata, pv_power_df\n",
    "    \n",
    "pv_metadata, pv_power_df = align_pv_system_ids(pv_metadata, pv_power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to the range [0, 1]\n",
    "pv_power_min = pv_power_df.min()\n",
    "pv_power_max = pv_power_df.max()\n",
    "\n",
    "pv_power_df -= pv_power_min\n",
    "pv_power_df /= pv_power_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop systems which are producing over night\n",
    "NIGHT_YIELD_THRESHOLD = 0.4\n",
    "night_hours = list(range(21, 24)) + list(range(0, 4))\n",
    "bad_systems = np.where(\n",
    "    (pv_power_df[pv_power_df.index.hour.isin(night_hours)] > NIGHT_YIELD_THRESHOLD).sum()\n",
    ")[0]\n",
    "bad_systems = pv_power_df.columns[bad_systems]\n",
    "print(len(bad_systems), 'bad systems found.')\n",
    "\n",
    "#ax = pv_power_df[bad_systems].plot(figsize=(40, 10), alpha=0.5)\n",
    "#ax.set_title('Bad PV systems');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df.drop(bad_systems, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Interpolate up to 15 minutes ahead.\n",
    "pv_power_df = pv_power_df.interpolate(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align again, after removing dud PV systems\n",
    "pv_metadata, pv_power_df = align_pv_system_ids(pv_metadata, pv_power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pv_power_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pv_power_df.plot(figsize=(40, 10), alpha=0.5, legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from torch.utils.data import Dataset\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECTANGLE_WIDTH_M = 128000 # in meters\n",
    "RECTANGLE_HEIGHT_M = RECTANGLE_WIDTH_M\n",
    "\n",
    "METERS_PER_PIXEL = 1000\n",
    "RECTANGLE_WIDTH_PIXELS = np.int(RECTANGLE_WIDTH_M / METERS_PER_PIXEL)\n",
    "RECTANGLE_HEIGHT_PIXELS = np.int(RECTANGLE_HEIGHT_M / METERS_PER_PIXEL)\n",
    "\n",
    "SAT_IMAGE_MEAN = 20.444992\n",
    "SAT_IMAGE_STD = 8.766013\n",
    "\n",
    "\n",
    "def get_rectangle(data_array, centre_x, centre_y, width=RECTANGLE_WIDTH_M, height=RECTANGLE_HEIGHT_M):\n",
    "    half_width = width / 2\n",
    "    half_height = height / 2\n",
    "\n",
    "    north = centre_y + half_height\n",
    "    south = centre_y - half_height\n",
    "    east = centre_x + half_width\n",
    "    west = centre_x - half_width\n",
    "\n",
    "    return data_array.loc[dict(\n",
    "        x=slice(west, east), \n",
    "        y=slice(north, south))]\n",
    "\n",
    "\n",
    "class SatelliteLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        index: pd.Series which maps from UTC datetime to full filename of satellite data.\n",
    "        _data_array_cache: The last lazily opened xr.DataArray that __getitem__ was asked to open.\n",
    "            Useful so that we don't have to re-open the DataArray if we're asked to get\n",
    "            data from the same file on several different calls.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_pattern):\n",
    "        self._load_sat_index(file_pattern)\n",
    "        self._data_array_cache = None\n",
    "        self._last_filename_requested = None\n",
    "        \n",
    "    def __getitem__(self, dt: datetime) -> xr.DataArray:\n",
    "        \"\"\"Returns lazily-opened DataArray\"\"\"\n",
    "        sat_filename = self.index[dt]\n",
    "        if sat_filename != self._last_filename_requested:\n",
    "            self._data_array_cache = xr.open_dataarray(sat_filename)\n",
    "            self._last_filename_requested = sat_filename\n",
    "        return self._data_array_cache.sel(time=dt)\n",
    "    \n",
    "    def close(self):\n",
    "        if self._data_array_cache is not None:\n",
    "            self._data_array_cache.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "        \n",
    "    def _load_sat_index(self, file_pattern):\n",
    "        \"\"\"Opens all satellite files in `file_pattern` and loads all their datetime indicies into self.index.\"\"\"\n",
    "        sat_filenames = glob(file_pattern)\n",
    "        sat_filenames.sort()\n",
    "        \n",
    "        n_filenames = len(sat_filenames)\n",
    "        sat_index = []\n",
    "        for i_filename, sat_filename in enumerate(sat_filenames):\n",
    "            if i_filename % 10 == 0 or i_filename == (n_filenames - 1):\n",
    "                print('\\r {:5d} of {:5d}'.format(i_filename + 1, n_filenames), end='', flush=True)\n",
    "            data_array = xr.open_dataarray(sat_filename, drop_variables=['x', 'y'])\n",
    "            sat_index.extend([(sat_filename, t) for t in data_array.time.values])\n",
    "\n",
    "        sat_index = pd.DataFrame(sat_index, columns=['filename', 'datetime']).set_index('datetime').squeeze()\n",
    "        assert not any(sat_index.index.duplicated())\n",
    "        self.index = sat_index.tz_localize('UTC')\n",
    "        \n",
    "    def get_rectangles_for_all_data(self, centre_x, centre_y, width=RECTANGLE_WIDTH_M, height=RECTANGLE_HEIGHT_M):\n",
    "        \"\"\"Iterate through all satellite filenames and load rectangle of imagery.\"\"\"\n",
    "        sat_filenames = np.sort(np.unique(self.index.values))\n",
    "        for sat_filename in sat_filenames:\n",
    "            data_array = xr.open_dataarray(sat_filename)\n",
    "            yield get_rectangle(data_array, time, centre_x, centre_y, width, height)\n",
    "        \n",
    "    def get_rectangle(self, time, centre_x, centre_y, width=RECTANGLE_WIDTH_M, height=RECTANGLE_HEIGHT_M):\n",
    "        data_array = self[time]\n",
    "        return get_rectangle(data_array, centre_x, centre_y, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sat_loader = SatelliteLoader(os.path.join(SATELLITE_DATA_PATH, '*.nc'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sat_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test get rectangle\n",
    "dt = pd.Timestamp('2019-02-21 10:15')\n",
    "pv_system_id = pv_metadata.index[1]\n",
    "x, y = pv_metadata.loc[pv_system_id][['x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sat_data = sat_loader.get_rectangle(time=dt, centre_x=x, centre_y=y) #, width=512000, height=512000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "crs = ccrs.TransverseMercator()\n",
    "ax = plt.axes(projection=crs)\n",
    "ax.coastlines(resolution='10m', alpha=0.5, color='pink')\n",
    "\n",
    "sat_data.plot.imshow(ax=ax, cmap='gray', origin='upper', add_colorbar=True)\n",
    "ax.scatter(x=x, y=y, alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test clearsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvlib\n",
    "from pvlib.location import Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pvlib_location(pv_system_id):\n",
    "    return Location(\n",
    "        latitude=pv_metadata['latitude'][pv_system_id],\n",
    "        longitude=pv_metadata['longitude'][pv_system_id],\n",
    "        tz='UTC',\n",
    "        name=pv_metadata['system_name'][pv_system_id])\n",
    "\n",
    "location = get_pvlib_location(pv_system_id)\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "pv_data_to_plot = pv_power_df[pv_system_id][dt - timedelta(hours=48):dt + timedelta(hours=48)]\n",
    "ax.plot(pv_data_to_plot, label='PV yield')\n",
    "#ax.plot((dt, dt), (0, 1), linewidth=1, color='black', label='datetime of image above')\n",
    "ax.set_title(dt)\n",
    "ax.set_ylim((0, 1))\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "clearsky = location.get_clearsky(pv_data_to_plot.index)\n",
    "lines = ax2.plot(clearsky)\n",
    "for line, label in zip(lines, clearsky.columns):\n",
    "    line.set_label(label);\n",
    "ax2.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align satellite datetime index with PV datetime index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_index = pv_power_df.index.intersection(sat_loader.index.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by datetimes when sun is shining!\n",
    "daylight_mask = location.get_clearsky(datetime_index)['ghi'] > 0\n",
    "datetime_index = datetime_index[daylight_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df = pv_power_df.reindex(datetime_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datetime_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_index.tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & test by days\n",
    "days = np.unique(datetime_index.date)\n",
    "len(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use every 5th day for testing\n",
    "testing_days = days[::5]\n",
    "len(testing_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_days = np.array(list(set(days) - set(testing_days)))\n",
    "training_days = np.sort(training_days)\n",
    "len(training_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_index_for_days(training_or_testing_days):\n",
    "    return datetime_index[pd.Series(datetime_index.date).isin(training_or_testing_days)]\n",
    "\n",
    "training_datetimes = get_datetime_index_for_days(training_days)\n",
    "testing_datetimes = get_datetime_index_for_days(testing_days)\n",
    "assert not set(training_datetimes).intersection(testing_datetimes)\n",
    "\n",
    "len(training_datetimes), len(testing_datetimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testing batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_full_array(size, fill_value=np.NaN, dtype=np.float16):\n",
    "    return np.full(shape=size, fill_value=fill_value, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_batch = {\n",
    "    'sat_images': new_full_array(\n",
    "        size=(TESTING_BATCH_SIZE, 1, RECTANGLE_WIDTH_PIXELS, RECTANGLE_HEIGHT_PIXELS),\n",
    "        dtype=np.float32),  # use float32 to minimise problems with normalisation\n",
    "    'pv_yield': new_full_array(\n",
    "        size=(TESTING_BATCH_SIZE, 1)),\n",
    "    'pv_system_id': np.zeros(shape=TESTING_BATCH_SIZE, dtype=np.int32),\n",
    "    'datetime_index': testing_datetimes[:TESTING_BATCH_SIZE]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dt in enumerate(testing_batch['datetime_index']):\n",
    "    # Randomly sample from PV systems which have data for this datetime\n",
    "    pv_data_for_dt = pv_power_df.loc[dt].dropna()\n",
    "    pv_system_id = np.random.choice(pv_data_for_dt.index)\n",
    "    pv_yield = pv_data_for_dt[pv_system_id]\n",
    "    \n",
    "    # Load satellite image\n",
    "    x, y = pv_metadata.loc[pv_system_id][['x', 'y']]\n",
    "    sat_data = sat_loader.get_rectangle(time=dt, centre_x=x, centre_y=y)\n",
    "    \n",
    "    # Put into super batch\n",
    "    testing_batch['sat_images'][i, 0] = sat_data.values\n",
    "    testing_batch['pv_yield'][i, 0] = pv_yield\n",
    "    testing_batch['pv_system_id'][i] = pv_system_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise satellite images\n",
    "testing_batch['sat_images'] -= SAT_IMAGE_MEAN\n",
    "testing_batch['sat_images'] /= SAT_IMAGE_STD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training super batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RECTANGLES_PER_SAT_IMAGE = 32\n",
    "N_DATETIMES_PER_SUPERBATCH = 4096\n",
    "\n",
    "SUPER_BATCH_SIZE = N_RECTANGLES_PER_SAT_IMAGE * N_DATETIMES_PER_SUPERBATCH\n",
    "BYTES_PER_PIXEL = 2  # float16\n",
    "size_of_each_image_mb = (RECTANGLE_HEIGHT_PIXELS * RECTANGLE_WIDTH_PIXELS * BYTES_PER_PIXEL) / 1E6\n",
    "super_batch_size_mb = size_of_each_image_mb * SUPER_BATCH_SIZE\n",
    "print('Size of super batch: {:8.1f} MB'.format(super_batch_size_mb))\n",
    "print('                     {:6d}   examples'.format(SUPER_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CPU super batch from individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cpu_super_batch = {\n",
    "    'sat_images': new_full_array(\n",
    "        size=(SUPER_BATCH_SIZE, 1, RECTANGLE_WIDTH_PIXELS, RECTANGLE_HEIGHT_PIXELS), \n",
    "        dtype=np.float32),  # use float32 to minimise issues with normalisation\n",
    "    'pv_yield': new_full_array(\n",
    "        size=(SUPER_BATCH_SIZE, 1)),\n",
    "    'pv_system_id': np.zeros(shape=SUPER_BATCH_SIZE, dtype=np.int32),\n",
    "    'datetime_index': np.zeros(shape=SUPER_BATCH_SIZE, dtype='datetime64[s]')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HALF_RECTANGLE_WIDTH_M = RECTANGLE_WIDTH_M / 2\n",
    "HALF_RECTANGLE_HEIGHT_M = RECTANGLE_HEIGHT_M / 2\n",
    "\n",
    "def load_data_into_cpu_super_batch():\n",
    "    # Get datetimes for superbatch by randomly sampling\n",
    "    super_batch_datetimes = np.random.choice(training_datetimes.to_numpy(dtype=object), size=N_DATETIMES_PER_SUPERBATCH)\n",
    "    super_batch_datetimes = np.sort(super_batch_datetimes)\n",
    "    super_batch_datetimes = pd.DatetimeIndex(super_batch_datetimes)\n",
    "    cpu_super_batch['datetime_index'] = np.zeros(shape=SUPER_BATCH_SIZE, dtype='datetime64[s]')\n",
    "    \n",
    "    # Load satellite data and PV data\n",
    "    for image_i, dt in enumerate(super_batch_datetimes):\n",
    "        print('\\r{:6d} of {:d}'.format(image_i+1, N_DATETIMES_PER_SUPERBATCH), end='', flush=True)\n",
    "\n",
    "        # Randomly sample from PV systems which have data for this datetime\n",
    "        pv_data_for_dt = pv_power_df.loc[dt].dropna()\n",
    "        replace = len(pv_data_for_dt) < N_RECTANGLES_PER_SAT_IMAGE\n",
    "        pv_system_ids = np.random.choice(pv_data_for_dt.index, size=N_RECTANGLES_PER_SAT_IMAGE, replace=replace)\n",
    "        locations = pv_metadata.loc[pv_system_ids][['x', 'y']]\n",
    "\n",
    "        # Get bounding box\n",
    "        north = locations['y'].max() + HALF_RECTANGLE_HEIGHT_M\n",
    "        south = locations['y'].min() - HALF_RECTANGLE_HEIGHT_M\n",
    "        west = locations['x'].min() - HALF_RECTANGLE_WIDTH_M\n",
    "        east = locations['x'].max() + HALF_RECTANGLE_WIDTH_M\n",
    "\n",
    "        # Load satellite images\n",
    "        data_array = sat_loader[dt]\n",
    "        data_array = data_array.loc[dict(\n",
    "            x=slice(west, east), \n",
    "            y=slice(north, south))]\n",
    "        data_array = data_array.load()\n",
    "\n",
    "        example_i = image_i * N_RECTANGLES_PER_SAT_IMAGE\n",
    "        for pv_system_id, row in locations.iterrows():\n",
    "            sat_data = get_rectangle(data_array, centre_x=row.x, centre_y=row.y)\n",
    "            pv_yield = pv_data_for_dt[pv_system_id]\n",
    "\n",
    "            # Put into super batch\n",
    "            cpu_super_batch['sat_images'][example_i, 0] = sat_data.values\n",
    "            cpu_super_batch['pv_yield'][example_i, 0] = pv_yield\n",
    "            cpu_super_batch['pv_system_id'][example_i] = pv_system_id\n",
    "            cpu_super_batch['datetime_index'][example_i] = dt.to_numpy()  # TODO: Maybe move this to a vectorised solution?\n",
    "            example_i += 1\n",
    "            \n",
    "    cpu_super_batch['datetime_index'] = pd.DatetimeIndex(cpu_super_batch['datetime_index'], tz='UTC')\n",
    "\n",
    "    # Normalise satellite images\n",
    "    cpu_super_batch['sat_images'] -= SAT_IMAGE_MEAN\n",
    "    cpu_super_batch['sat_images'] /= SAT_IMAGE_STD\n",
    "\n",
    "    print()\n",
    "    return cpu_super_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cpu_super_batch = load_data_into_cpu_super_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(batch_dict, i):\n",
    "    plt.imshow(batch_dict['sat_images'][i, 0].astype(np.float32))\n",
    "    print(batch_dict['datetime_index'][i])\n",
    "    print('PV yield', batch_dict['pv_yield'][i])\n",
    "    for key in ['clearsky', 'hours_of_day']:\n",
    "        try:\n",
    "            print(key, batch_dict[key][i])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "plot(cpu_super_batch, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute hour of day and clearsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURS_OF_DAY_MEAN = 11.628418\n",
    "HOURS_OF_DAY_STD = 4.1584363\n",
    "\n",
    "def compute_hour_of_day(batch_dict):\n",
    "    hours_of_day = batch_dict['datetime_index'].hour.values.astype(np.float32)\n",
    "    hours_of_day -= HOURS_OF_DAY_MEAN\n",
    "    hours_of_day /= HOURS_OF_DAY_STD\n",
    "    batch_dict['hours_of_day'] = hours_of_day[:, np.newaxis]\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_super_batch = compute_hour_of_day(cpu_super_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_batch = compute_hour_of_day(testing_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEARSKY_MEAN = np.array([373.1623 , 538.70374,  80.82757], dtype=np.float32)\n",
    "CLEARSKY_STD = np.array([268.6872  , 254.62102 ,  42.651264], dtype=np.float32)\n",
    "\n",
    "def compute_clearsky(batch_dict):\n",
    "    n_examples = len(batch_dict['datetime_index'])\n",
    "    clearsky = np.full(shape=(n_examples, 3), fill_value=np.NaN, dtype=np.float32)\n",
    "    pv_ids_and_datetimes = pd.DataFrame(\n",
    "        {'pv_system_id': batch_dict['pv_system_id'], \n",
    "         'datetime_index': batch_dict['datetime_index']})\n",
    "    \n",
    "    for pv_system_id, df in pv_ids_and_datetimes.groupby('pv_system_id'):\n",
    "        dt_index = pd.DatetimeIndex(df['datetime_index'])\n",
    "        location = get_pvlib_location(pv_system_id)\n",
    "        clearsky_for_location = location.get_clearsky(dt_index)\n",
    "        clearsky[df.index] = clearsky_for_location.values\n",
    "\n",
    "    assert not any(np.isnan(clearsky).flatten())\n",
    "    \n",
    "    clearsky -= CLEARSKY_MEAN\n",
    "    clearsky /= CLEARSKY_STD\n",
    "    \n",
    "    batch_dict['clearsky'] = clearsky\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cpu_super_batch = compute_clearsky(cpu_super_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "testing_batch = compute_clearsky(testing_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU super batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_full_tensor(size, fill_value=np.NaN, dtype=torch.float16, device='cuda'):\n",
    "    return torch.full(size=size, fill_value=fill_value, dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_super_batch = {\n",
    "    'sat_images': new_full_tensor(\n",
    "        size=(SUPER_BATCH_SIZE, 1, RECTANGLE_WIDTH_PIXELS, RECTANGLE_HEIGHT_PIXELS)),\n",
    "    'pv_yield': new_full_tensor(size=(SUPER_BATCH_SIZE, 1)),\n",
    "    'hours_of_day': new_full_tensor(size=(SUPER_BATCH_SIZE, 1)),\n",
    "    'clearsky': new_full_tensor(size=(SUPER_BATCH_SIZE, 3))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_superbatch_to_gpu(cpu_super_batch):\n",
    "    for k, v in cpu_super_batch.items():\n",
    "        if k in ['datetime_index', 'pv_system_id']:\n",
    "            gpu_super_batch[k] = copy(v)\n",
    "        else:\n",
    "            try:\n",
    "                gpu_super_batch[k].copy_(torch.HalfTensor(v))\n",
    "            except:\n",
    "                print('Problem with', k)\n",
    "                raise\n",
    "\n",
    "    return gpu_super_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gpu_super_batch = move_superbatch_to_gpu(cpu_super_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_super_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move testing batch into GPU memory\n",
    "for key in ['sat_images', 'pv_yield', 'hours_of_day', 'clearsky']:\n",
    "    testing_batch[key] = torch.cuda.HalfTensor(testing_batch[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_proportion=0.1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=16, kernel_size=5)\n",
    "        HOURS_OF_DAY_CHANNELS = 1\n",
    "        CLEARSKY_CHANNELS = 3\n",
    "        self.fc1 = nn.Linear(16 * 29 * 29, 120)\n",
    "        self.fc2 = nn.Linear(120 + HOURS_OF_DAY_CHANNELS + CLEARSKY_CHANNELS, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_proportion)\n",
    "\n",
    "    def forward(self, x, hour_of_day, clearsky):\n",
    "        #x = self.dropout_layer(x)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x is now <batch_size>, 6, 62, 62.  \n",
    "        # 62 is 124 / 2.  124 is the 128-dim input - 4\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x is now <batch_size>, 16, 29, 29\n",
    "        x = x.view(-1, 16 * 29 * 29)\n",
    "        # x is now <batch_size>, 16 x 29 x 29\n",
    "        x = self.dropout_layer(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_layer(x)\n",
    "        x = torch.cat((x, hour_of_day, clearsky), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_func = nn.MSELoss()\n",
    "mae_loss_func = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_mae_losses = []\n",
    "test_losses = []\n",
    "test_mae_losses = []\n",
    "\n",
    "training_index_len_minus_1 = SUPER_BATCH_SIZE - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BATCH_SIZE = 128\n",
    "\n",
    "BATCHES_PER_EPOCH = int(SUPER_BATCH_SIZE / TRAINING_BATCH_SIZE)\n",
    "STATS_PERIOD = int(BATCHES_PER_EPOCH / 4)\n",
    "N_EPOCHS = 7\n",
    "N_LOADS = 7\n",
    "N_BATCHES_TO_TRAIN = BATCHES_PER_EPOCH * N_EPOCHS\n",
    "\n",
    "TESTING_INPUTS = testing_batch['sat_images']\n",
    "TESTING_TARGET = testing_batch['pv_yield']\n",
    "TESTING_HOURS_OF_DAY = testing_batch['hours_of_day']\n",
    "TESTING_CLEARSKY = testing_batch['clearsky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i_load in range(N_LOADS):\n",
    "    print('loading', i_load, 'of', N_LOADS)\n",
    "    t0 = time.time()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_mae = 0.0\n",
    "    for i_batch in range(N_BATCHES_TO_TRAIN):\n",
    "        print('\\rBatch: {:4d} of {}'.format(i_batch + 1, N_BATCHES_TO_TRAIN), end='', flush=True)\n",
    "\n",
    "        # Create batch\n",
    "        batch_index = np.random.randint(low=0, high=training_index_len_minus_1, size=TRAINING_BATCH_SIZE)\n",
    "        inputs = gpu_super_batch['sat_images'][batch_index]\n",
    "        hours_of_day_for_batch = gpu_super_batch['hours_of_day'][batch_index]\n",
    "        clearsky_for_batch = gpu_super_batch['clearsky'][batch_index]\n",
    "        target = gpu_super_batch['pv_yield'][batch_index]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        net.train()\n",
    "        outputs = net(inputs, hours_of_day_for_batch, clearsky_for_batch)\n",
    "        train_loss = loss_func(outputs, target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += train_loss.item()\n",
    "\n",
    "        # MAE\n",
    "        train_mae = mae_loss_func(outputs, target)\n",
    "        running_train_mae += train_mae.item()\n",
    "\n",
    "        # print statistics\n",
    "        if i_batch == 0 or i_batch % STATS_PERIOD == STATS_PERIOD - 1:    # print every STATS_PERIOD mini-batches\n",
    "            t1 = time.time()\n",
    "\n",
    "            # Train loss\n",
    "            if i_batch == 0:\n",
    "                mean_train_loss = running_train_loss\n",
    "                mean_train_mae = running_train_mae\n",
    "            else:\n",
    "                mean_train_loss = running_train_loss / STATS_PERIOD\n",
    "                mean_train_mae = running_train_mae / STATS_PERIOD\n",
    "\n",
    "            train_losses.append(mean_train_loss)\n",
    "            train_mae_losses.append(mean_train_mae)\n",
    "\n",
    "            # Test loss\n",
    "            net.eval()\n",
    "            test_outputs = net(TESTING_INPUTS, TESTING_HOURS_OF_DAY, TESTING_CLEARSKY)\n",
    "            test_loss = loss_func(test_outputs, TESTING_TARGET).item()\n",
    "            test_losses.append(test_loss)\n",
    "            test_mae = mae_loss_func(test_outputs, TESTING_TARGET).item()\n",
    "            test_mae_losses.append(test_mae)\n",
    "\n",
    "            print(\n",
    "                '\\n        time =   {:.2f} milli seconds per batch.\\n'\n",
    "                '   train loss = {:8.5f}\\n'\n",
    "                '    train MAE = {:8.5f}\\n'\n",
    "                '    test loss = {:8.5f}\\n'\n",
    "                '     test MAE = {:8.5f}'.format(\n",
    "                    ((t1 - t0) / STATS_PERIOD) * 1000,\n",
    "                    mean_train_loss, \n",
    "                    mean_train_mae,\n",
    "                    test_loss,\n",
    "                    test_mae\n",
    "                ))\n",
    "            running_train_loss = 0.0\n",
    "            running_train_mae = 0.0\n",
    "            t0 = time.time()\n",
    "          \n",
    "    print()\n",
    "    print('Loading new data!')\n",
    "    cpu_super_batch = load_data_into_cpu_super_batch()\n",
    "    cpu_super_batch = compute_hour_of_day(cpu_super_batch)\n",
    "    cpu_super_batch = compute_clearsky(cpu_super_batch)\n",
    "    gpu_super_batch = move_superbatch_to_gpu(cpu_super_batch)\n",
    "\n",
    "print()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(20, 10))\n",
    "\n",
    "ax1.plot(test_losses, label='testing')\n",
    "ax1.plot(train_losses, label='training')\n",
    "ax1.set_title('MSE (training objective)')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(test_mae_losses, label='testing')\n",
    "ax2.plot(train_mae_losses, label='training')\n",
    "ax2.set_title('MAE')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "plt.imshow(inputs[i, 0].cpu().numpy().astype(np.float32))\n",
    "dt = gpu_super_batch['datetime_index'][batch_index]\n",
    "dt[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearsky_for_batch[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(target.cpu(), clearsky_for_batch[:, 0].cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield",
   "language": "python",
   "name": "predict_pv_yield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

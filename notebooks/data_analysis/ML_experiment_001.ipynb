{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import rasterio.warp as rasteriowarp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SATELLITE_DATA_PATH = os.path.expanduser('~/data/EUMETSAT/reprojected_subsetted/')\n",
    "PV_DATA_FILENAME = os.path.expanduser('~/data/pvoutput.org/UK_PV_timeseries_batch.nc')\n",
    "PV_METADATA_FILENAME = os.path.expanduser('~/data/pvoutput.org/UK_PV_metadata.csv')\n",
    "\n",
    "DST_CRS = {\n",
    "    'ellps': 'WGS84',\n",
    "    'proj': 'tmerc',  # Transverse Mercator\n",
    "    'units': 'm'  # meters\n",
    "}\n",
    "\n",
    "# Geospatial boundary in Transverse Mercator (meters)\n",
    "SOUTH = 5513500\n",
    "NORTH = 6613500\n",
    "WEST =  -889500\n",
    "EAST =   410500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and convert PV metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metadata = pd.read_csv(PV_METADATA_FILENAME, index_col='system_id')\n",
    "pv_metadata.dropna(subset=['longitude', 'latitude'], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2548"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lat lons to Transverse Mercator\n",
    "pv_metadata['x'], pv_metadata['y'] = rasteriowarp.transform(\n",
    "    src_crs={'init': 'EPSG:4326'},\n",
    "    dst_crs=DST_CRS,\n",
    "    xs=pv_metadata['longitude'].values,\n",
    "    ys=pv_metadata['latitude'].values)\n",
    "\n",
    "# Filter 3 PV systems which apparently aren't in the UK!\n",
    "pv_metadata = pv_metadata[\n",
    "    (pv_metadata.x >= WEST) &\n",
    "    (pv_metadata.x <= EAST) &\n",
    "    (pv_metadata.y <= NORTH) &\n",
    "    (pv_metadata.y >= SOUTH)]\n",
    "\n",
    "len(pv_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalise PV power data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 2.22 s, total: 15.7 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pv_power = xr.load_dataset(PV_DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_selected = pv_power.loc[dict(datetime=slice('2018-06-01', '2019-07-01'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df = pv_power_selected.to_dataframe().dropna(axis='columns', how='all')\n",
    "pv_power_df = pv_power_df.clip(lower=0, upper=5E7)\n",
    "pv_power_df.columns = [np.int64(col) for col in pv_power_df.columns]\n",
    "pv_power_df = pv_power_df.tz_localize('Europe/London').tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pv_power\n",
    "del pv_power_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of hand-crafted cleaning\n",
    "# TODO: Is this still relevant?\n",
    "pv_power_df[30248][:'2019-01-03'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to the range [0, 1]\n",
    "pv_power_df -= pv_power_df.min()\n",
    "pv_power_df /= pv_power_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop systems which are producing over night\n",
    "NIGHT_YIELD_THRESHOLD = 0.4\n",
    "night_hours = list(range(21, 24)) + list(range(0, 4))\n",
    "bad_systems = np.where(\n",
    "    (pv_power_df[pv_power_df.index.hour.isin(night_hours)] > NIGHT_YIELD_THRESHOLD).sum()\n",
    ")[0]\n",
    "bad_systems = pv_power_df.columns[bad_systems]\n",
    "\n",
    "#ax = pv_power_df[bad_systems].plot(figsize=(40, 10), alpha=0.5)\n",
    "#ax.set_title('Bad PV systems');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power_df.drop(bad_systems, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Interpolate\n",
    "pv_power_df = pv_power_df.interpolate(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pv_power_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pv_power_df.plot(figsize=(40, 10), alpha=0.5, legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the metadata in the same order as the PV power data\n",
    "pv_metadata = pv_metadata.reindex(pv_power_df.columns, axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rectangle(data_array, time, centre_x, centre_y, width=128000, height=128000):\n",
    "    half_width = width / 2\n",
    "    half_height = height / 2\n",
    "\n",
    "    north = centre_y + half_height\n",
    "    south = centre_y - half_height\n",
    "    east = centre_x + half_width\n",
    "    west = centre_x - half_width\n",
    "\n",
    "    data = data_array.loc[dict(\n",
    "        x=slice(west, east), \n",
    "        y=slice(north, south))]\n",
    "\n",
    "    MEAN = 20.444992\n",
    "    STD = 8.766013\n",
    "    data = data - MEAN\n",
    "    data = data / STD \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "class SatelliteLoader(Dataset):\n",
    "    def __init__(self, file_pattern):\n",
    "        self._load_sat_index(file_pattern)\n",
    "        self._data_array_cache = None\n",
    "        self._last_filename_requested = None\n",
    "        \n",
    "    def __getitem__(self, dt):\n",
    "        sat_filename = self.index[dt]\n",
    "        if self._data_array_cache is None or sat_filename != self._last_filename_requested:\n",
    "            self._data_array_cache = xr.open_dataarray(sat_filename)\n",
    "            self._last_filename_requested = sat_filename\n",
    "        return self._data_array_cache\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "        \n",
    "    def _load_sat_index(self, file_pattern):\n",
    "        sat_filenames = glob(file_pattern)\n",
    "        sat_filenames.sort()\n",
    "        \n",
    "        n_filenames = len(sat_filenames)\n",
    "        sat_index = []\n",
    "        for i_filename, sat_filename in enumerate(sat_filenames):\n",
    "            if i_filename % 10 == 0:\n",
    "                print('\\r {:5d} of {:5d}'.format(i_filename, n_filenames), end='', flush=True)\n",
    "            data_array = xr.open_dataarray(sat_filename, drop_variables=['x', 'y'])\n",
    "            sat_index.extend([(sat_filename, t) for t in data_array.time.values])\n",
    "\n",
    "        sat_index = pd.DataFrame(sat_index, columns=['filename', 'datetime']).set_index('datetime').squeeze()\n",
    "        self.index = sat_index.tz_localize('UTC')\n",
    "        \n",
    "    def get_rectangles_for_all_data(self, centre_x, centre_y, width=128000, height=128000):\n",
    "        sat_filenames = np.sort(np.unique(self.index.values))\n",
    "        for sat_filename in sat_filenames:\n",
    "            data_array = xr.open_dataarray(sat_filename)\n",
    "            yield get_rectangle(data_array, time, centre_x, centre_y, width, height)\n",
    "        \n",
    "    def get_rectangle(self, time, centre_x, centre_y, width=128000, height=128000):\n",
    "        return get_rectangle(self[time], time, centre_x, centre_y, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sat_loader = SatelliteLoader(os.path.join(SATELLITE_DATA_PATH, '*.nc'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sat_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test get rectangle\n",
    "dt = pd.Timestamp('2019-02-21 10:15')\n",
    "pv_system_id = pv_metadata.index[0]\n",
    "x, y = pv_metadata.loc[pv_system_id][['x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sat_data = sat_loader.get_rectangle(time=dt, centre_x=x, centre_y=y) #, width=512000, height=512000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "crs = ccrs.TransverseMercator()\n",
    "ax = plt.axes(projection=crs)\n",
    "ax.coastlines(resolution='10m', alpha=0.5, color='pink')\n",
    "\n",
    "img = sat_data.isel(time=10).plot.imshow(ax=ax, cmap='gray', origin='upper', add_colorbar=True)\n",
    "path_collection = ax.scatter(x=x, y=y, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "ax.plot(\n",
    "    pv_power_df[pv_system_id][dt - timedelta(hours=6):dt + timedelta(hours=6)], \n",
    "    label='PV yield')\n",
    "ax.plot((dt, dt), (0, 1), linewidth=1, color='black', label='datetime of image above')\n",
    "ax.legend()\n",
    "ax.set_title(dt)\n",
    "ax.set_ylim((0, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load all satellite data rectangles into RAM\n",
    "dims = OrderedDict()\n",
    "dims['time'] = sat_loader.index.index.values\n",
    "dims['y'] = sat_data.y\n",
    "dims['x'] = sat_data.x\n",
    "\n",
    "shape = [len(values) for values in dims.values()]\n",
    "print('Creating huge numpy array!', flush=True)\n",
    "data = np.zeros(shape, dtype=np.float16)\n",
    "print('Setting to NaN', flush=True)\n",
    "data[:, :, :] = np.NaN\n",
    "print('Creating huge DataArray!', flush=True)\n",
    "sat_data_master = xr.DataArray(\n",
    "    data,\n",
    "    coords=dims,\n",
    "    dims=dims.keys(),\n",
    "    name='HRV')\n",
    "del data, dims, shape\n",
    "\n",
    "for data_array in sat_loader.get_rectangles_for_all_data(centre_x=x, centre_y=y):\n",
    "    print('\\r', data_array.time.values[0], flush=True, end='')\n",
    "    sat_data_master.loc[data_array.time.values, :, :] = data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVDataset(Dataset):\n",
    "    def __init__(self, train=False):\n",
    "        self.pv_data = pv_power_df[pv_system_id].dropna()\n",
    "        self.x, self.y = pv_metadata.loc[pv_system_id][['x', 'y']]\n",
    "        self.datetime_index = self.pv_data.index.intersection(sat_loader.index.index)\n",
    "        self.pv_data = self.pv_data.loc[self.datetime_index]\n",
    "        \n",
    "        train_test_split = int(len(self.datetime_index) / 5)\n",
    "        if train:\n",
    "            self.datetime_index = self.datetime_index[train_test_split:]\n",
    "        else:\n",
    "            self.datetime_index = self.datetime_index[:train_test_split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datetime_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dt = self.datetime_index[idx]\n",
    "        pv_data = self.pv_data.loc[dt]\n",
    "        sat_data = sat_data_master.sel(time=dt)\n",
    "        sat_data = sat_data.values\n",
    "        sat_data = sat_data[np.newaxis]\n",
    "        pv_data = np.array([pv_data])\n",
    "        \n",
    "        if any(np.isnan(pv_data).flatten()) or any(np.isnan(sat_data).flatten()):\n",
    "            print('NaNs detected!  Trying again!')\n",
    "            new_idx = np.random.randint(low=0, high=len(self)-1)\n",
    "            return self[new_idx]\n",
    "        \n",
    "        return sat_data, pv_data\n",
    "        #return torch.cuda.FloatTensor(sat_data), torch.cuda.FloatTensor(pv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pv_dataset_test = PVDataset(train=False)\n",
    "len(pv_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pv_dataset_train = PVDataset(train=True)\n",
    "len(pv_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sat_data, pv_data = pv_dataset_test[2350]\n",
    "print(sat_data.shape)\n",
    "print(pv_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(pv_dataset_train, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 29 * 29, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x is now <batch_size>, 6, 62, 62.  \n",
    "        # 62 is 124 / 2.  124 is the 128-dim input - 4\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x is now <batch_size>, 16, 29, 29\n",
    "        x = x.view(-1, 16 * 29 * 29)\n",
    "        # x is now <batch_size>, 16 x 29 x 29\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "STATS_PERIOD = 50\n",
    "PRINT_TIMINGS = True\n",
    "\n",
    "losses = []\n",
    "n = len(dataloader)\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    t0 = ti0 = time.time()\n",
    "    for i_batch, (inputs, target) in enumerate(dataloader):\n",
    "        print('\\r Epoch:', epoch, 'Batch:', i_batch, 'of', n, end='', flush=True)\n",
    "        ti1 = time.time()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = torch.cuda.FloatTensor(inputs)  # TODO: This might crash kernel.  Maybe move this back to PVDataset\n",
    "        target = torch.cuda.FloatTensor(targets)\n",
    "        ti2 = time.time()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ti3 = time.time()\n",
    "        \n",
    "        \n",
    "        if PRINT_TIMINGS:\n",
    "            print()\n",
    "            print('ti1 - ti0', ti1 - ti0)\n",
    "            print('ti2 - ti1', ti2 - ti1)\n",
    "            print('ti3 - ti2', ti3 - ti2)\n",
    "            print()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i_batch % STATS_PERIOD == STATS_PERIOD - 1:    # print every 2000 mini-batches\n",
    "            t1 = time.time()\n",
    "            mean_loss = running_loss / STATS_PERIOD\n",
    "            losses.append(mean_loss)\n",
    "            print('\\n[%d, %5d] loss: %.3f; time= %.2f s' %\n",
    "                  (epoch + 1, i_batch + 1, mean_loss, t1 - t0))\n",
    "            running_loss = 0.0\n",
    "            t0 = time.time()\n",
    "            \n",
    "        ti0 = time.time()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_i = 0\n",
    "for inputs, target in DataLoader(pv_dataset_test, batch_size=64, shuffle=False, drop_last=True):\n",
    "    test_i += 1\n",
    "    if test_i == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 30\n",
    "plt.imshow(inputs[i, 0].cpu(), origin='upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[i, 0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[i, 0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax.plot(output[:, 0].detach().cpu(), label='net output')\n",
    "ax.plot(target[:, 0].detach().cpu(), label='target')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield",
   "language": "python",
   "name": "predict_pv_yield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
